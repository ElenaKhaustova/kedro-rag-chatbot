# Kedro RAG Chatbot
This project demonstrates how to use Kedro to create a Retrieval-Augmented Generation (RAG)-based chatbot.

The chatbot is designed to assist users with Kedro-related questions by leveraging historical Q&A data from our [Kedro Slack](https://kedro-org.slack.com/archives/C03RKP2LW64) support channel. It creates a vector store from Slack conversations and employs a Generative AI-based agent to retrieve relevant context and generate accurate responses.

See the demo on [YouTube](https://www.youtube.com/watch?v=rgmANk-QwYg).

## Features
- Extracts Q&A data from Slack conversations
- Converts text data into embeddings and stores them in a vector database
- Implements a retrieval-augmented chatbot using LangChain and OpenAI
- Interactive CLI interface for user interaction
- Compares RAG-based answers with responses from a standard LLM (without context retrieval)
- Saves interaction logs, including user questions, retrieved context, and chatbot responses

## Setup

### 1. Clone the Repository
```console
https://github.com/ElenaKhaustova/kedro-rag-chatbot.git
cd kedro-rag-chatbot
```

### 2. Install Dependencies

```console
pip install -r requirements.txt
```
### 3. Add API Credentials

Create a `credentials.yml` file and place it in the `conf/base/` directory with the following format:
```yaml
openai:
  openai_api_base: <openai-api-base>
  openai_api_key: <openai-api-key>
```

### 4. Verify Data Availability

The necessary raw data for a test run is already included in `data/01_raw`.

## Running the Project

### Step 1: Create the Vector Store

This step processes the Slack Q&A data and stores embeddings in a vector database.

```console
kedro run -p create_vector_store
```

### Step 2: Run the Chatbot Agent

This step initializes the AI agent, allowing it to query the vector store and generate responses.

```console
kedro run -t agent_rag
```

**Note:** to run `agent_rag` pipeline we use `agent_rag` tag to reuse some nodes from `create_vector_store` pipeline.

## Usage

Once the chatbot is running, you can interact with it via the CLI. For each question you ask, the chatbot will provide:

1. A response generated by the RAG agent using retrieved context.
2. A response from a standard LLM without context retrieval.

This allows you to compare the effectiveness of retrieval-augmented generation versus a general-purpose model.

After exiting the loop, all questions asked, retrieved context, and generated answers are saved in `data/08_reporting/output.md`.
